---
title: "Meta Time Series Analysis"
author: "Ryan_Gan"
date: "May 8, 2017"
output: html_document
---

## Background

This is a pooled analysis of state and year specific time-series evaluating the association between wildfire smoke exposure and cardipulmonary outcomes in the Western United States. 

### Notable differences between Washington and Oregon:

*Health Data:* Washington CHARS data captures persons who visited a hosptial in the state. Oregon data is insurance based and captures insured persons billed for a medical visit in the state of Oregon. 

*Smoke Data:* Washington smoke estimates were in general more accurate when compared with surface monitor estimates comapred to Oregon.

Both state datasets were limited to only emergency room visits (billed for Oregon, observed for Washington).

```{r libaries, message = F, warning = F, echo = F}
library(tidyverse)
library(lme4)
library(broom)
library(maps)
# loading gganimate (David Robinson's package on Github)
# still experimental and likely has some bugs
#library(gganimate)
#library(animation)

# two-stage distributed lag model libraries
library(dlnm)
library(mvmeta)
library(splines)
```

Read in meta time series data.

```{r read data, message = F, warning = F, echo = F}
meta_ts <- read_csv("./data/meta_timeseries.csv") %>% 
  # creating 10 unit increase var of geo smoke and day of week
  mutate(geo_smk10 = geo_smk_pm/10,
         day = weekdays(date),
         state_county = paste(state, county, sep = "_")) %>% 
  # creating lag variables for a crude distributed lag
  group_by(state_county) %>% 
  # create smoke and lag variables
  mutate(smoke0 = ifelse(geo_smk_pm > 0, 1, 0),
         smoke5 = ifelse(geo_smk_pm > 5, 1, 0),
         smoke10 = ifelse(geo_smk_pm > 10, 1, 0),
         smoke15 = ifelse(geo_smk_pm > 15, 1, 0), 
         # lag variable
         geo_smk_lag1 = lag(geo_smk10, 1, order_by = state_county),
         geo_smk_lag2 = lag(geo_smk10, 2, order_by = state_county),
         geo_smk_lag3 = lag(geo_smk10, 3, order_by = state_county),
         geo_smk_lag4 = lag(geo_smk10, 4, order_by = state_county),
         geo_smk_lag5 = lag(geo_smk10, 5, order_by = state_county),
         geo_smk_lag6 = lag(geo_smk10, 6, order_by = state_county),
         geo_smk_lag7 = lag(geo_smk10, 7, order_by = state_county)) %>% 
  ungroup()

# smoke wave days (at least 2 consecutive days in the 98th percentile of PM)
quantile(meta_ts$geo_wt_pm, probs = c(0.95,0.98,0.99, 0.999))
# following Coco's smoke wave day, the 98th percentile for PM2.5 concentrations
# would be >26.9

smoke_wave <- meta_ts %>% 
  select(state, county, date, geo_wt_pm) %>% 
  mutate(high_pm_day = ifelse(geo_wt_pm > 26.9, 1, 0)) %>% 
  group_by(state, county) %>% 
  # identify consecutive days
  mutate(smoke_wave = ifelse(lag(high_pm_day, 
    order_by = county)==1 & high_pm_day ==1, 1, 0)) %>% 
  select(-high_pm_day, -geo_wt_pm)

# merge in smokewave day
meta_ts <- meta_ts %>% 
  right_join(smoke_wave, by = c("state", "county", "date"))

# washington
wa_ts <- meta_ts %>% filter(state == "WA")
# oregon
or_ts <- meta_ts %>% filter(state == "OR")

# count of state outcome timeseries
state_outcome_rate <- meta_ts %>% group_by(state, date) %>% 
  # remove columns I don't want to calculate
  select(-fips, -county) %>% 
  # add up outcomes in each county
  summarise_at(vars(population, n_obs:broken_arm_n), sum) %>% 
  mutate_at(vars( n_obs:broken_arm_n), funs((./population*100000))) 

# average of environmental measures, joined with outcomes
state_smoke <- meta_ts %>% group_by(state, date) %>% 
  select(-fips, -county) %>% 
  summarise_at(vars(wrf_pm:wrf_temp), funs(mean, median, min, max)) %>% 
  full_join(state_outcome_rate, by = c("state", "date"))

```

I also incorporated the smoke-wave day approach by Coco Liu where I identified the 98th percentile of the observed PM2.5 values for the GWR estimates (without background PM2.5 subtracted off). Then I set the binary cutoff of at least two days of levels above this 98th percentile as a smoke wave day.

Comparing smoke-wave approach to our method of defining smoke days

```{r smoke days, message=F, warning=F, echo = F}
# only 150 smoke wave days
xtabs(~smoke_wave,  meta_ts)
# if we compare to our geo smoke > 10, we'd add 362 extra days
xtabs(~smoke_wave + smoke10, meta_ts)
# if we used smoke >15 we'd add 164 extra days 
xtabs(~smoke_wave + smoke15, meta_ts)
```

The "smoke wave" definition captures 150 days in Oregon counties where smoke is likely to have been present. Using our cutoff of at least 10 ug/m^3 or 15 ug/m^3 on the smoke PM2.5 variables, we add at 362 and 164 additional days, respectively. Probably the >15 ug/m^3 cutoff balances sensitivity and specificity the best. Although this would be an interesting analysis to look at if we knew "true" smoke in certain areas.
 
## Descriptive Plots

First step is to determine if there are any notable differences between the states. Going to aggregate counties by state (averaging health outcomes and smoke) and split states to Washington and Oregon.

### Visualizing Exposure Time Series

Small multiples of smoke exposure series by smoke estimation method for each county within each state.

#### Washington 2012 Smoke Time Series

```{r washington smoke, message = F, warning = F, echo = F, results = "asis"}

# small multiples dataset
wa_smk <- wa_ts %>% 
  select(county, date, wrf_smk_pm, krig_smk_pm, geo_smk_pm) %>% 
  gather(key = smk_method, value = pm, -date, - county)
  
  
wa_smk_plot <- ggplot(wa_smk, aes(x = date, y = pm)) +
  geom_point(aes(colour = smk_method), alpha = 0.8, size = 0.8) +
  ggtitle("Washington: 2012 Fire Season Smoke PM2.5") +
  facet_wrap(~county) +
  ylab("Smoke PM2.5 ug/m^3") +
  xlab("Year: 2012") +
  theme_bw()

wa_smk_plot

```

#### Oregon 2013 Smoke Time Series

```{r oregon smoke, message = F, warning = F, echo = F, results = "asis"}

# small multiples dataset
or_smk <- or_ts %>% 
  select(county, date, wrf_smk_pm, krig_smk_pm, geo_smk_pm) %>% 
  gather(key = smk_method, value = pm, -date, -county)
  
  
or_smk_plot <- ggplot(or_smk, aes(x = date, y = pm)) +
  geom_point(aes(colour = smk_method), alpha = 0.8, size = 0.8) +
  facet_wrap(~county) +
  ggtitle("Oregon: 2013 Fire Season Smoke PM2.5") +
  ylab("Smoke PM2.5 ug/m^3") +
  xlab("Year: 2013") +
  theme_bw()

or_smk_plot

```

Seems like the southwest portion of Oregon was affected by smoke where the rest of the state did not have much smoke exposure. The smoke exposure timeframe also appears to have a smaller window of time on which it affected the area.

#### Maps of Number of Days Counties Affected by Smoke

*Washington*

```{r static wash smoke map, message=F, warning=F, echo = F, results="asis"}

# data wrangle ----
#first step is to summarise the counts of smoke days in each county 
wa_smk_count <- meta_ts %>% filter(state=="WA") %>% 
  group_by(county) %>% 
  # summing up smoke days
  summarise(smk_wave_n = sum(smoke_wave), smk_day0 = sum(smoke0), 
            smk_day5 = sum(smoke5), smk_day10 = sum(smoke10),
            smk_day15 = sum(smoke15)) %>% 
  # county needs to be lowercase to join to maps dataframe
  mutate(county = tolower(county))

# maps package will be loaded to pull map data and create a dataframe
wa_county_df <- map_data("county", "washington")

# join county counts to map df
wa_smk_df <- wa_county_df %>% 
  right_join(wa_smk_count, by = c("subregion" = "county")) %>% 
  # small multiples of different smoke cutoff methods
  gather(key = smk_cut_method, value = smoke_days, -long:-subregion) %>% 
  # filter out smoke day 0 since it's not very specific
  filter(smk_cut_method != "smk_day0") %>% 
  # rename smoke day cutoffs 
  mutate(smk_cut_method = 
    ifelse(smk_cut_method == "smk_day5","Smoke PM2.5 > 5 ug/m^3",
    ifelse(smk_cut_method == "smk_day10", "Smoke PM2.5 > 10 ug/m^3",
    ifelse(smk_cut_method == "smk_day15", "Smoke PM2.5 > 15 ug/m^3",
    ifelse(smk_cut_method == "smk_wave_n", "Smoke Wave", NA)))),
    # preserve smoke method order for small multiple
    smk_cut_method = factor(smk_cut_method, 
      levels = c("Smoke PM2.5 > 5 ug/m^3", "Smoke PM2.5 > 10 ug/m^3", 
                 "Smoke PM2.5 > 15 ug/m^3", "Smoke Wave")))

# map ----

# smoke days where GWR smoke > 15
smoke_map <- ggplot(wa_smk_df, aes(x=long, y=lat, group=group)) +
  # fill with number of smoke days
  geom_polygon(aes(fill = smoke_days), alpha = 0.7) +
  scale_fill_gradient("Smoke-Impacted Days",
                      low = "#0b486b", high = "#f56217") +
  # add county path on top
  geom_path() +
  facet_wrap(~smk_cut_method) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Washington: Counties affected by smoke 2012") +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    strip.background = element_rect(colour=NA, fill=NA))

smoke_map

```


*Oregon*

```{r static oregon smoke map, message=F, warning=F, echo = F,  results="asis"}

# data wrangle ----
#first step is to summarise the counts of smoke days in each county 
or_smk_count <- meta_ts %>% filter(state=="OR") %>% 
  group_by(county) %>% 
  # summing up smoke days
  summarise(smk_wave_n = sum(smoke_wave), smk_day0 = sum(smoke0), 
            smk_day5 = sum(smoke5), smk_day10 = sum(smoke10),
            smk_day15 = sum(smoke15)) %>% 
  # county needs to be lowercase to join to maps dataframe
  mutate(county = tolower(county))

# maps package will be loaded to pull map data and create a dataframe
or_county_df <- map_data("county", "oregon")

# join county counts to map df
or_smk_df <- or_county_df %>% 
  right_join(or_smk_count, by = c("subregion" = "county")) %>% 
  # small multiples of different smoke cutoff methods
  gather(key = smk_cut_method, value = smoke_days, -long:-subregion) %>% 
  # filter out smoke day 0 since it's not very specific
  filter(smk_cut_method != "smk_day0") %>% 
  # rename smoke day cutoffs 
  mutate(smk_cut_method = 
    ifelse(smk_cut_method == "smk_day5","Smoke PM2.5 > 5 ug/m^3",
    ifelse(smk_cut_method == "smk_day10", "Smoke PM2.5 > 10 ug/m^3",
    ifelse(smk_cut_method == "smk_day15", "Smoke PM2.5 > 15 ug/m^3",
    ifelse(smk_cut_method == "smk_wave_n", "Smoke Wave", NA)))),
    # preserve smoke method order for small multiple
    smk_cut_method = factor(smk_cut_method, 
      levels = c("Smoke PM2.5 > 5 ug/m^3", "Smoke PM2.5 > 10 ug/m^3", 
                 "Smoke PM2.5 > 15 ug/m^3", "Smoke Wave")))

# map ----

# smoke days where GWR smoke > 15
smoke_map <- ggplot(or_smk_df, aes(x=long, y=lat, group=group)) +
  # fill with number of smoke days
  geom_polygon(aes(fill = smoke_days), alpha = 0.7) +
  scale_fill_gradient("Smoke-Impacted Days",
                      low = "#0b486b", high = "#f56217") +
  # add county path on top
  geom_path() +
  facet_wrap(~smk_cut_method) +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Oregon: Counties affected by smoke") +
  theme(panel.background = element_rect(fill = 'white', colour = 'black'),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    strip.background = element_rect(colour=NA, fill=NA))

smoke_map

```


### Visualizing Health Outcome Time Series

Comparison between Oregon and Washington aggregated health outcomes over time.

```{r state outcome trends, message = F, warning = F, echo = F, results = "asis"}
# I want CVD and respiratory rates by each state so I can make a 4 panel
state_cvd_resp <- state_outcome_rate %>% 
  select(state, date, cvd_n, resp_n) %>% 
  gather(key = outcome, value = n, -date, -state)
  

count_plot <- ggplot(state_cvd_resp, aes(x = date, y = n)) +
  geom_point() +
  facet_wrap(~state+outcome, scales = "free") +
  ylab("Outcome Rate Per 100k Person") +
  xlab("Date") +
  theme_bw()


count_plot
```

CVD rates in Oregon for 2013 and Washington 2012 were similar, with no clear trend in the outcome rate over time. 

All respiratory outcomes were slightly different between Washington and Oregon. It looks like Oregon has almost twice the respiratory ER rate compared to Washington. This could mean some difference in respiratory rates between states, or more likely could have something to do with the different data sources. I'll need to think about this more. Justifies accounting for state though. Also, both states appear to have trends in respiratory ER visits where there is a decline in summer months, with an increase in the rate around the fall. Oregon has a pretty pronounced seasonal variation. 

It would be good to take a look at state-specific outcome time-series. However, in general it is probably safe to assume that there is a temporal pattern in the respiratory outcomes and no clear temporal pattern with CVD outcomes.

### Washington Health Outcomes Time Series

```{r wash health ts plot, warning=F, message=F, echo = F, results='asis'}

# small multiples dataset
wa_health <- wa_ts %>% 
  select(county, date, population, n_obs:broken_arm_n) %>% 
  gather(key = outcome, value = events, -date, -county, -population) %>% 
  mutate(rate_per_100k = (events/population)*100000) %>% 
  filter(outcome == "resp_n" | outcome == "cvd_n")

#glimpse(wa_health)

# plot of rates
count_plot <- ggplot(wa_health, aes(x=date, y=events)) +
  geom_point(aes(colour = outcome), alpha = 0.8, size = 0.8) +
  facet_wrap(~county, scales = "free") +
  ggtitle("Washington: 2012 Fire Season Respiratory and CVD Rates") +
  ylab("Rates Per 100k") +
  xlab("Year:2012") +
  theme_bw()

count_plot
```


*Oregon*

```{r oregon health ts plot, message=F, warning=F, echo = F,  results='asis'}
# small multiples dataset
or_health <- or_ts %>% 
  select(county, date, population, n_obs:broken_arm_n) %>% 
  gather(key = outcome, value = events, -date, -county, -population) %>% 
  mutate(rate_per_100k = (events/population)*100000) %>% 
  filter(outcome == "resp_n" | outcome == "cvd_n")

# plot of rates
count_plot <- ggplot(or_health, aes(x=date, y=events)) +
  geom_point(aes(colour = outcome), alpha = 0.8, size = 0.8) +
  facet_wrap(~county, scales = "free") +
  ggtitle("Oregon: 2013 Fire Season Respiratory and CVD Rates") +
  ylab("Rates Per 100k") +
  xlab("Year:2013") +
  theme_bw()

count_plot

```


### Scatter Plot of Outcome Rates and Geo Smoke PM~2.5~

Visualizing rates of outcomes by 10ug/m^3 increasing Geoweighted smoke PM~2.5~.

```{r scatter plot, message=F, warning=F, echo = F, results="asis"}
# calclating rates per 100k
rate_df <- meta_ts %>% mutate_at(
    which(colnames(meta_ts)=="n_obs"):which(colnames(meta_ts)=="broken_arm_n"),
    funs((./population)*100000)) %>% 
  mutate_at(c("wrf_smk_pm", "krig_smk_pm", "geo_smk_pm"),
    funs(./10)) %>% 
  select(state, county, date, geo_smk_pm, geo_wt_pm,
  which(colnames(meta_ts)=="n_obs"):
  which(colnames(meta_ts)=="broken_arm_n")) %>% 
  gather(key = outcome, value = rate_100k, -state, 
         -county, -date, -geo_smk_pm, -geo_wt_pm) %>% 
  filter(outcome != "n_obs")
  

# plot of pm2.5 every 10 units and asthma
ggplot(rate_df, aes(x = geo_smk_pm, y = log(rate_100k+0.1))) +
  geom_point(aes(colour = state), alpha = 0.2,  size = 1) + 
  scale_colour_manual(values = c("red", "blue")) +
  # truncated y axis
  #scale_y_continuous(limits = c(0, 1.5)) +
  # fit line for Washington
  geom_smooth(data = filter(rate_df, state == "WA"), 
              method=lm, formula = y~x, se = F, colour = "blue") +
  # fit line for Oregon
  geom_smooth(data = filter(rate_df, state == "OR"),
              method=lm, formula = y~x, se = F, colour = "red") +
  facet_wrap(~outcome, scale  = "free") +
  ylab("Log Outcome Rate per 100,000 Persons") +
  xlab("GWR Smoke PM2.5 per 10ug/m^3") +
  theme_bw()  

```

It looks like there is a linear relationship with smoke and asthma (expected), maybe with cerebrovascular disease, COPD, and respiratory outcomes.

### Standard Regression Models

Same-day estimates of association between smoke via GWR and outcomes. 
County and state treated as a fixed effect covariate.

```{r standard analysis, message=F, warning=F, echo = F, results="asis"}
# create empty dataframe
rr_df <- data.frame(matrix(nrow = 12, ncol = 4))
colnames(rr_df) <- c("outcome", "rel_risk", "lower_95", "upper_95")

# vector of outcomes
outcome_list <- colnames(meta_ts[, which(colnames(meta_ts)=="resp_n"):
    which(colnames(meta_ts)=="broken_arm_n")])

# names 
outcome_names <- c('All Respiratory', 'Asthma', 'COPD', 'Pneumonia', 
  'Acute Bronchitis', 'Cardiovascular Disease', 'Arrhythmia', 
  'Cerebrovascular Disease', 'Heart Failure', 'Ischemic Heart Disease',
  'Myocardial Infarction', 'Broken Arm')

# populate outcome column
rr_df[,1] <- factor(outcome_names, levels=unique(outcome_names))

meow <- lapply(outcome_list, function(x){

 smk_est <- tidy(glm(as.formula(paste0(x, 
    "~ geo_smk10 + wrf_temp + state + county + 
    day + offset(log(population))")), 
    meta_ts, family="quasipoisson"))[2, ]
 
 rr <- c(round(exp(smk_est[1,2]),3),
         round(exp(smk_est[1,2]-1.96*smk_est[1,3]),3),
         round(exp(smk_est[1,2]+1.96*smk_est[1,3]),3))
 
}) # end apply

for(i in 1:nrow(rr_df)){
  rr_df[i,2:4] <- meow[[i]]
}

# add in a grouping variable
rr_df$group <- NA
rr_df[1:5, 5] <- "Respiratory"
rr_df[6:11, 5] <- "CVD"
rr_df[12, 5] <- "Broken Arm"

# tried apply function and it might have been easier to just use a loop
# as I needed a for loop to fill the list in to the dataframe
# code is more compact though
# I should check speed (or make a custom function to do this one day)

ggplot(rr_df, aes(x=outcome, y = rel_risk, colour = group)) +
  geom_point() +
  geom_errorbar(aes(ymin=lower_95, ymax=upper_95), width = 0.2) +
  geom_hline(yintercept = 1, linetype = 2, colour = "red") +
  theme(
   panel.background = element_rect(fill = 'white', colour = 'black'),
   panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(),
   axis.text.x = element_text(angle = 90, vjust = .75)) +
  ylab("Relative Risk") +
  xlab("Outcome") +
  ggtitle("Relative Risk for a 10ug/m^3 increase PM2.5 estimated via GWR Smoke")
```

Smoke-wave associations with outcomes.

```{r standard analysis smoke wave, message=F, warning=F, echo = F, results="asis"}
# create empty dataframe
rr_df <- data.frame(matrix(nrow = 12, ncol = 4))
colnames(rr_df) <- c("outcome", "rel_risk", "lower_95", "upper_95")

# vector of outcomes
outcome_list <- colnames(meta_ts[, which(colnames(meta_ts)=="resp_n"):
    which(colnames(meta_ts)=="broken_arm_n")])

# names 
outcome_names <- c('All Respiratory', 'Asthma', 'COPD', 'Pneumonia', 
  'Acute Bronchitis', 'Cardiovascular Disease', 'Arrhythmia', 
  'Cerebrovascular Disease', 'Heart Failure', 'Ischemic Heart Disease',
  'Myocardial Infarction', 'Broken Arm')

# populate outcome column
rr_df[,1] <- factor(outcome_names, levels=unique(outcome_names))

meow <- lapply(outcome_list, function(x){

 smk_est <- tidy(glm(as.formula(paste0(x, 
    "~ smoke_wave + wrf_temp + state + county + 
    day + offset(log(population))")), 
    meta_ts, family="quasipoisson"))[2, ]
 
 rr <- c(round(exp(smk_est[1,2]),3),
         round(exp(smk_est[1,2]-1.96*smk_est[1,3]),3),
         round(exp(smk_est[1,2]+1.96*smk_est[1,3]),3))
 
}) # end apply

for(i in 1:nrow(rr_df)){
  rr_df[i,2:4] <- meow[[i]]
}

# add in a grouping variable
rr_df$group <- NA
rr_df[1:5, 5] <- "Respiratory"
rr_df[6:11, 5] <- "CVD"
rr_df[12, 5] <- "Broken Arm"

# tried apply function and it might have been easier to just use a loop
# as I needed a for loop to fill the list in to the dataframe
# code is more compact though
# I should check speed (or make a custom function to do this one day)

ggplot(rr_df, aes(x=outcome, y = rel_risk, colour = group)) +
  geom_point() +
  geom_errorbar(aes(ymin=lower_95, ymax=upper_95), width = 0.2) +
  geom_hline(yintercept = 1, linetype = 2, colour = "red") +
  theme(
   panel.background = element_rect(fill = 'white', colour = 'black'),
   panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(),
   axis.text.x = element_text(angle = 90, vjust = .75)) +
  ylab("Relative Risk") +
  xlab("Outcome") +
  ggtitle("Relative risk for exposure to a smoke-wave")
```


### Mixed Model

Continous smoke via GWR. County and state treated as a random effect.

```{r random effects smk, message=F, warning=F, echo = F, results="asis"}
# create empty dataframe
rr_df <- data.frame(matrix(nrow = 12, ncol = 4))
colnames(rr_df) <- c("outcome", "rel_risk", "lower_95", "upper_95")

# vector of outcomes
outcome_list <- colnames(meta_ts[, which(colnames(meta_ts)=="resp_n"):
    which(colnames(meta_ts)=="broken_arm_n")])

# names 
outcome_names <- c('All Respiratory', 'Asthma', 'COPD', 'Pneumonia', 
  'Acute Bronchitis', 'Cardiovascular Disease', 'Arrhythmia', 
  'Cerebrovascular Disease', 'Heart Failure', 'Ischemic Heart Disease',
  'Myocardial Infarction', 'Broken Arm')

# populate outcome column
rr_df[,1] <- factor(outcome_names, levels=unique(outcome_names))

meow <- lapply(outcome_list, function(x){

 smk_est <- tidy(glmer(as.formula(paste0(x, 
    "~ geo_smk10 + wrf_temp + (1|county) + 
    (1|state) + offset(log(population))")), 
    meta_ts, family="poisson"))[2, ]
 
 rr <- c(round(exp(smk_est[1,2]),3),
         round(exp(smk_est[1,2]-1.96*smk_est[1,3]),3),
         round(exp(smk_est[1,2]+1.96*smk_est[1,3]),3))
 
}) # end apply

for(i in 1:nrow(rr_df)){
  rr_df[i,2:4] <- meow[[i]]
}

# add in a grouping variable
rr_df$group <- NA
rr_df[1:5, 5] <- "Respiratory"
rr_df[6:11, 5] <- "CVD"
rr_df[12, 5] <- "Broken Arm"

# tried apply function and it might have been easier to just use a loop
# as I needed a for loop to fill the list in to the dataframe
# code is more compact though
# I should check speed (or make a custom function to do this one day)

ggplot(rr_df, aes(x=outcome, y = rel_risk, colour = group)) +
  geom_point() +
  geom_errorbar(aes(ymin=lower_95, ymax=upper_95), width = 0.2) +
  geom_hline(yintercept = 1, linetype = 2, colour = "red") +
  theme(
   panel.background = element_rect(fill = 'white', colour = 'black'),
   panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(),
   axis.text.x = element_text(angle = 90, vjust = .75)) +
  ylab("Relative Risk") +
  xlab("Outcome") +
  ggtitle("Relative Risk for a 10ug/m^3 increase PM2.5 estimated via GWR Smoke")
```

## Interaction

Trying out interaction to see if anything is there.

```{r interaction exploration}
summary(meta_ts)
int_mod <- glmer(cvd_n ~ geo_wt_pm + smoke10 + geo_wt_pm*smoke10 +  
  (1|state_county), data=meta_ts,family="poisson")
summary(int_mod)
```

There might be evidence, but could be sample size challenged here. 

### Two-Stage Distributed Lag 

Two-stage distributed lag runs a county specific model and then pools the county-specific lag estimates in one final model.

For these analyses, I assume a linear relationship between smoke and outcomes (given sparse data and the scatter/fit plots above), and fit a natural spline for the lagged response. I decided on looking at lags up to 7 days following the initial smoke event.

Set up list of counties to run through two-stage distributed lag.

Distributed lag setup.

```{r two stage dlnm setup, message = F, echo = F, warning=F}
# create vector of state_county
counties <- as.character(unique(meta_ts$state_county))

# create dataframe list of each county
county_list_df <- lapply(counties, function(x) 
  meta_ts[meta_ts$state_county==x,])
# assign each element of the list object the county name
names(county_list_df) <- counties
# calculate length of counties (75)
m <- length(counties)

# GWR PM2.5 ranges
smk_range <- t(sapply(county_list_df, 
  function(x) range(x$geo_smk10,na.rm=T)))
# smk_range

# population range
pop_range <- t(sapply(county_list_df, 
  function(x) range(x$population,na.rm=T)))
# pop_range
# -----

# function to compute the QAIC (quasi-poisson AIC)
fqaic <- function(model) {
  loglik <- sum(dpois(model$y,model$fitted.values,log=TRUE))
  phi <- summary(model)$dispersion
  qaic <- -2*loglik + 2*summary(model)$df[3]*phi
  return(qaic)
}

# assuming linear relationship between smoke and outcomes
argvar <- list(fun="lin")

# fitting a natural spline of the lag function
degree_freedom <- 4
arglag <- list(fun="ns", df=degree_freedom)
# I'm not certain on lag yet, but maybe a week makes sense
lag <- 7

# overall cumulative summaries
# columns need to match degrees of freedom for lag
# 4 degrees of freedom for the spline
cumulative_counties <- matrix(NA, length(county_list_df), 
  as.numeric(degree_freedom), 
  dimnames=list(counties,paste("b",seq(degree_freedom),sep="")))

names(cumulative_counties) <- counties

# (co)variance matrices
counties_cov <- vector("list",length(county_list_df))
names(counties_cov) <- counties

# I also want to make a matrix of county-level models that fail to converge
converge <- matrix(NA, length(county_list_df), 1,
  dimnames=list(counties, "converge"))

# qaic
qaic_val <- 0
```


```{r qaic model fit loop, message=F, warning=F, echo = F, results="asis"}

# first stage ----
# I'm not certain on time lag yet, but maybe a week makes sense
lag <- 7

# create outcome list to loop through
outcome_names <- colnames(meta_ts[,which(colnames(meta_ts)=="resp_n"):
  which(colnames(meta_ts)=="broken_arm_n")])
# copd is having issues; i suspect counties with no events is the problem
# empty vector to fill with error messages of counties where models can be run
# mostly due to no variation in the outcome (i.e. all 0s in the time series)

fit_matrix <- matrix(NA, nrow=0, ncol=6)
colnames(fit_matrix) <- c("outcome", "exposure", "fit_type", "df", "aic",
                           "n_mod_conv")

system.time(
  
# 1st loop: loop through outcomes
for(q in seq(outcome_names)){

# 2nd loop: vary degrees of freedom for lag knots
# set up empty matrix to assess model fit
fit_mat <- matrix(NA, nrow = 3, ncol = 6) 

  # 2 df will not work for a spline, going from 3 to 6 df
  for(h in 3:5){ # start loop for df

    # fitting a natural spline for lag with 5 knots?
    fit_mat[h-2, 4] <- degree_freedom <- h
    arglag <- list(fun="ns", df=degree_freedom)
  
    # overall cumulative summaries
    # columns need to match degrees of freedom for lag
    # q: do i need to make this object multiple times or just fill it in?
    cumulative_counties <- matrix(NA, length(county_list_df), 
      as.numeric(degree_freedom), 
      dimnames=list(counties,paste("b",seq(degree_freedom),sep="")))
    
    names(cumulative_counties) <- counties
    
    # (co)variance matrices
    counties_cov <- vector("list",length(county_list_df))
    names(counties_cov) <- counties
    
    # I also want to make a matrix of county-level models that fail to converge
    converge <- matrix(NA, length(county_list_df), 1,
      dimnames=list(counties, "converge"))
    
    # empty table to log degrees of freedom and AIC for each outcome
    
    # qaic
    qaic_val <- 0

    # first stage ----
    # loop for each county
    for(i in seq(county_list_df)) {
      # if error in following code, move on to next county
      tryCatch({
      # load county-specific dataframe
      sub <- county_list_df[[i]]
      
      # run only if there is at least one observation of outcome in ts
      #if(max(sub[, which(colnames(sub)==outcome_names[[q]])]) > 2){

      # define the cross-basis ----
      suppressWarnings({
      # cross-basis for smoke
      smk_cb <- crossbasis(sub$geo_smk10,lag=lag,argvar=argvar,arglag=arglag)
      # cross-basis for temp
      temp_cb <- crossbasis(sub$wrf_temp, lag = lag, argvar = argvar,
                            arglag = arglag)
      })
  
      # run the first stage county-specific models
      county_model <- glm(as.formula(paste0(outcome_names[q], 
        "~ smk_cb + temp_cb + offset(log(population))")), 
        family = "quasipoisson", sub)
      
      # alert warning if there are convergence issues
      converge[i,] <- county_model$converged
      
      # reduction to summary results ---
      # prediction summary for smoke 10 ug/m^3 
      # uses cross reduce function
      suppressWarnings({
        cr_counties <- crossreduce(smk_cb, county_model, type = "var", value = 1)

        })

      # store results in matrix ---
      # daily lag results
      cumulative_counties[i,] <- coef(cr_counties)
      counties_cov[[i]] <- vcov(cr_counties)
      
      # Q-AIC
      qaic_val[i] <- fqaic(county_model)
      
        }, error = function(e){
          # print(names(county_list_df[i]))
          }
        ) # end try catch

      } # end loop for county estimates

    # error at hood river; using trycatch to keep going
    #which(counties[]=="OR_Hood River")
    
    # fill in the fit matrix ----
    fit_mat[,1] <- outcome_names[q]
    fit_mat[,2] <- exposure <- "wrf_smk10"
    fit_mat[,3] <- "ns"
    # subset qaic to only models that converged
    # sum the grand q aic and fill fit matrix
    fit_mat[h-2,5] <- round(sum(qaic_val[!is.na(converge)[]==T]),2)
    # fill in number of county models that successfully converge
    fit_mat[h-2, 6] <- length(which(converge==T))
    sum(complete.cases(qaic_val))

    } # end model fit loop
  
  fit_matrix <- rbind(fit_matrix, fit_mat)

}) # end outcome loop and system time

# now that we have a range of df/knots, I want to filter to the minimum aic
# for each outcome and print out that value in a table

lag_spline_best_fit <- as_tibble(fit_matrix) %>%  
  group_by(outcome) %>% 
  slice(which.min(aic))

# kable
knitr::kable(lag_spline_best_fit, caption = paste0("Distributed lag spline",
  "degree of freedom best fit by AIC for two-stage approach"))
```

This table shows the optimal degrees of freedom/knots for the natural spline. Now that the degrees of freedom of the lag spline has been decided by lowest AIC, I'll run the loop for actual results. It also shows how many counties out of the two states were able to converge. Only asthma and broken arm fit better with a 4 DF spline. All other outcomes fit best with a 3 DF spline.

Note: Oregon county Hood River was not able to run, so I marked as unable to converge.

Following models only adjust for temperature and have a population offset.

```{r distributed lag results, message=F, warning=F, echo = F, results="asis"}

# set estimation method for multivariate approach
# reml estimation method
method <- "reml"

# create a dataframe that contains the distributed lag estimates for each outcome
dist_lag_df <- as_tibble(matrix(NA,  nrow = 0, ncol = 5))
colnames(dist_lag_df) <- c("outcome", "lag", "rel_risk", 
                                  "lower_95", "upper_95")

# produce results using fit matrix

# 1st loop: loop through outcomes
for(j in 1:length(lag_spline_best_fit$outcome)){

    # easiest to set out the parameters from the start
    outcome <- lag_spline_best_fit$outcome[j]
    degree_freedom <- as.numeric(lag_spline_best_fit$df[j])
  
    arglag <- list(fun="ns", df=degree_freedom)
  
    # overall cumulative summaries
    # columns need to match degrees of freedom for lag
    # q: do i need to make this object multiple times or just fill it in?
    cumulative_counties <- matrix(NA, length(county_list_df), 
      as.numeric(degree_freedom), 
      dimnames=list(counties,paste("b",seq(degree_freedom),sep="")))
    
    names(cumulative_counties) <- counties
    
    # (co)variance matrices
    counties_cov <- vector("list",length(county_list_df))
    names(counties_cov) <- counties
    
    # I also want to make a matrix of county-level models that fail to converge
    converge <- matrix(NA, length(county_list_df), 1,
      dimnames=list(counties, "converge"))

    # first stage loop for each county ----
    for(i in seq(county_list_df)) {
      # if error in following code, move on to next county using trycatch
      tryCatch({
      # load county-specific dataframe
      sub <- county_list_df[[i]]
      
      # define the cross-basis
      suppressWarnings({
      # cross-basis for smoke
      smk_cb <- crossbasis(sub$geo_smk10,lag=lag,argvar=argvar,arglag=arglag)
      # cross-basis for temp
      temp_cb <- crossbasis(sub$wrf_temp, lag = lag, argvar = argvar,
                            arglag = arglag)
      })
  
      # run the first stage county-specific models
      county_model <- glm(as.formula(paste0(outcome, 
        "~ smk_cb + temp_cb + offset(log(population))")), 
        family = "quasipoisson", sub)
      
      # alert warning if there are convergence issues
      converge[i,] <- county_model$converged
      
      # reduction to summary results ---
      # prediction summary for smoke 10 ug/m^3 
      # uses cross reduce function
      suppressWarnings({
        cr_counties <- crossreduce(smk_cb, county_model, type = "var", value = 1)
        })

      # store results in matrix ---
      # daily lag results
      cumulative_counties[i,] <- coef(cr_counties)
      counties_cov[[i]] <- vcov(cr_counties)
        }, error = function(e){
          #print(names(county_list_df[i]))
          }) # end try catch

      } # end loop for county estimates

  # second stage multivariate meta analysis ---- 
  # For counties with outcomes that will not even run, set their converge
  # from missing to False for subsetting
  converge[is.na(converge)] <- FALSE
    
  # first step is to reduce the county-specific matrix to those that converged
  meta_counties <- cumulative_counties[converge[]==T, ]
  # reduce covariance matrix to models that converged
  meta_cov <- counties_cov[converge[]==T]

  # overall pooled summary for asthma model
  cumulative_mv_meta <- mvmeta(meta_counties~1, meta_cov, method = method)

  # create bases for prediction ----
  # bases of smoke lag (7 days with 10 intervals in each)
  xlag <- 0:70/10
  blag <- do.call("onebasis",c(list(x=xlag),attr(smk_cb,"arglag")))

  # county-specific first stage summaries
  county_lag_estimates <- apply(cumulative_counties,1, function(x) exp(blag%*%x))
  
  # prediction for a grid of lag days
  # predictor-specific summaries for 10 ug/m^3 of smoke
  pooled_cp <- crosspred(blag, coef=coef(cumulative_mv_meta), 
    vcov=vcov(cumulative_mv_meta), model.link="log", at = xlag)

  # create empty dataframe to fill with estimates
  # fill lag relationship with outcome tibble
  # empty matrix to fill in distributed lag values
  outcome_smk_lag_df <- as_tibble(matrix(NA,  nrow = length(xlag), ncol = 5))
  # assign outcome name
  colnames(outcome_smk_lag_df) <- c("outcome", "lag", "rel_risk", 
                                  "lower_95", "upper_95")

  # fill dataframe with values
  outcome_smk_lag_df[,1] <- outcome
  outcome_smk_lag_df[,2] <- xlag
  outcome_smk_lag_df[,3] <- pooled_cp$matRRfit
  outcome_smk_lag_df[,4] <- pooled_cp$matRRlow
  outcome_smk_lag_df[,5] <- pooled_cp$matRRhigh

  # bind works better than trying to figure out how to fill many rows
  dist_lag_df <- rbind(dist_lag_df, outcome_smk_lag_df)

}


ggplot(dist_lag_df, aes(x=lag, y=rel_risk)) +
  geom_line(colour = "blue") +
  geom_ribbon(aes(ymin = lower_95, ymax = upper_95), fill = "blue", alpha = 0.3) + 
  scale_y_continuous(limits = c(0.9, 1.15)) +
  scale_x_continuous(breaks = c(seq(0, 7,by=1))) +
  geom_hline(yintercept = 1, linetype = 2, colour = "red") +
  facet_wrap(~ outcome) +
  ggtitle(paste0("Distributed lag association between a 10 ug/m^3",
                 " increase in GWR smoke PM2.5 and cardiopulmonary outcomes")) +
  ylab("Relative Risk for a 10 ug/m^3 increase in GWR smoke (fixed)") +
  xlab("Lagged Days") +
  theme_bw()

```

These distributed lag plots show the cumulative association of GWR smoke exposure on cardiopulmonary outcomes. I think the are cumulative associations. I thought I called the right estimate from the crossprediction function from the dlnm package, but they were exactly the same as the same day estimates. So I'm not quite sure if it's entirely right. I think under certain situations, they are the same, according to some notes in the package. I'll look in to this. Ander may be able to help too. These interpretations need work (first step would be to figure out if this is a cumulative delayed effect or a delay effect after a single day of exposure).

My general interpreation is that ER respiratory visits are increased on the same day of smoke exposure, decay a bit to baseline levels, and then increase maybe 5 to 7 days after exposure. The most pronounced association with increasing smoke is for the outcome of asthma. Where on the same day of elevated exposure to smoke, a 10 ug/m^3 increase in PM2.5 is associated with a ~10% increase in asthma ER visits. This increased risk slowly decays over the next couple days back to referent levels, but may increase again 4 to 6 days of repeated smoke exposure. COPD ER visits appear to increase by maybe ~3% after increased smoke exposure, and maintain that increased risk up to a week after. Pneumonia also follows a similar pattern. 

Risk for CVD ER visits may also be elevated on the first day of smoke exposure, return to baseline, but then may be meaningfully elevated roughly 6 days after exposre. As for the specific CVD outcomes, both the risk of IHD and myocardial infarction may be increased around 6 days after a 10 ug/m^3 increase in smoke exposure.


### Some notes to improve this work as we go along:

Consider adding urgent care visits as well. We thought maybe urgent care might be slighty different in Oregon so did not include it, but it would give us more events in counties. Also consider adding a weekday or day covariate to the two-stage lag models. I did not because I feared even more counties would not converge if we did. Weekend indicator might be a better choice though if DF in the county-level models are an issue.

I decided to look at lags only for a week after. Not sure we can go longer than a week given the amount of time-series data we have. 

Had some convergence issues for county-specific models. I need to try and figure out a way to find the models that won't converge. Ander had said this same process can be done in a mixed model rather than a two-stage process. That might help me avoid excluding counties that don't converge in the first stage and use more available data.

Next step would be to try and better understand what's going on, and extend to other outcomes. I believe it would also be useful in the grand scheme of the project to look at age groups, but more importantly different age groups. I can get counts by age groups. We will need to see how population is parsed out in categories for the census which would define our population offset (my categories will depend on census categories).

Add Colorado counties and as many years as possible. Add 2015 Washington. Clean code once Ander approves of the statistical process so Rish can feed in mortality counts.

### Estimate Distributed Lags Without DLNM package
Create own basis expansion and distributed lag model.

```{r basis function with asthma, echo=F, warning=F}
# reduce to complete cases
meta_ts2 <- meta_ts %>% filter(complete.cases(.)) %>% 
  # transforming wrf_temp to avoid eigen issues
  mutate(wrf_temp_sd = wrf_temp/sd(wrf_temp))


# output matrix of smoke and lag variables
smk_matrix <- as.matrix(meta_ts2[, c(28, 35:41)])

# define basis b using natural spline function
b <- ns(0:7, df = 4, intercept = T)

# row dimension of b should equal number of lag columns
dim(b)
dim(smk_matrix)
# multiply lagged pm matrix by basis
pm_b <- smk_matrix %*% b

# output asthma counts
asthma_counts <- meta_ts2$asthma_n
# fit mixed model
mod <- glmer(asthma_n ~  pm_b + wrf_temp_sd + (1|state_county) + offset(log(population)), meta_ts2, 
             family="poisson", control = glmerControl(optimizer = "bobyqa"))

summary(mod)
# check AIC
AIC(mod)
# output distributed lag beta parameters
dlparms <- mod@beta[2:5]
# estimate distributed lag values for each day
dl_estimates <- data.frame(estimate = b %*% dlparms)

# covariance matrix for knots (need to convert to matrix object)
cov_mat <- as.matrix(vcov(mod)[2:5,2:5]) 

# estimate variance of splines
dl_var <- b%*%cov_mat%*%t(b)

# calculate standard error for each lag value
dl_estimates$stderr <- sqrt(diag(dl_var))

# calculate lower and upper bounds
dl_estimates$lower_bound <- dl_estimates$estimate+dl_estimates$stderr*qt(1-0.975,
  df=df.residual(mod))  
dl_estimates$upper_bound <- dl_estimates$estimate+dl_estimates$stderr*qt(0.975,
  df=df.residual(mod))  


# estimate relative risk and 95%CI from beta
asthma_dl_rr <- dl_estimates %>% 
  mutate(estimate = exp(estimate),
         lower_bound = exp(lower_bound),
         upper_bound = exp(upper_bound),
         lag = 0:7)

# estimate cumulative effect
cumulative <- sum(dl_estimates$estimate)
cumulative

# estimate cumulative effect stnd error
cumulative_se <- sqrt(sum(dl_var))
cumulative_se

# estimate cumulative CI
cumulative_ci <- cumulative+cumulative_se*qt(c(1-0.975,0.975), 
                                             df=df.residual(mod))
cumulative_ci

exp(cumulative)
exp(cumulative_ci)

# plot 
p <- ggplot(asthma_dl_rr, aes(x=lag, y=estimate, 
                              ymin=lower_bound, ymax=upper_bound)) +
  geom_ribbon(alpha = 0.5) +
  geom_line(size=2)

p
```

Writing loop for distributed lag. I suspect that the number of df/knots will be the same with the mixed model, but I'll run it again to make sure. Benefit of the mixed model is that I can include some of the sparsely populated counties that failed to converge in the two stage approach. Although the level of detail is only by day so the lines to appear as smooth. Ander may be able to help with this.

```{r mixed mod aic fit, message=F, warning=F, results="asis"}
# outcomes to loop through
# create outcome list to loop through
outcome_names <- colnames(meta_ts2[,which(colnames(meta_ts2)=="resp_n"):
  which(colnames(meta_ts2)=="broken_arm_n")])

# create fit matrix
fit_matrix <- matrix(NA, nrow=0, ncol=5)
colnames(fit_matrix) <- c("outcome", "exposure", "fit_type", "df", "aic")

system.time(
  
# 1st loop: loop through outcomes
for(q in seq(outcome_names)){

# 2nd loop: vary degrees of freedom for lag knots
# set up empty matrix to assess model fit
fit_mat <- matrix(NA, nrow = 3, ncol = 5) 

# fill outcome name in first column
fit_mat[,1] <- outcome_names[q]
# fill in exposure (geo smoke)
fit_mat[,2] <- "geo_smk10"
# fit type
fit_mat[,3] <- "ns"
  
# 2 df will not work for a spline, going from 3 to 6 df
  for(h in 3:5){ # start loop for df

    # fitting a natural spline for lag with 5 knots?
    fit_mat[h-2, 4] <- degree_freedom <- h

    # define basis b using natural spline function
    b <- ns(0:7, df = degree_freedom, intercept = T)
    
    # multiply lagged pm matrix by basis
    pm_b <- smk_matrix %*% b
    
    # fit mixed model
    mod <- glmer(as.formula(paste0(outcome_names[q], "~pm_b+wrf_temp_sd+
      (1|state_county)+offset(log(population))")), meta_ts2, 
      family="poisson", control = glmerControl(optimizer = "bobyqa"))
    
    # fill AIC
    fit_mat[h-2,5] <- round(AIC(mod),2)

    } # end model fit loop
  
  fit_matrix <- rbind(fit_matrix, fit_mat)

}) # end outcome loop and system time

# now that we have a range of df/knots, I want to filter to the minimum aic
# for each outcome and print out that value in a table

lag_spline_best_fit <- as_tibble(fit_matrix) %>%  
  group_by(outcome) %>% 
  slice(which.min(aic))

# kable
knitr::kable(lag_spline_best_fit, caption = paste0("Distributed lag spline",
  "degree of freedom best fit by AIC for mixed model approach"))

```

Note I get some warnings (including max grade convergence issues), slightly different answers in best fit, and it takes about 18 minutes to run. Consider running in parallel.

I suspect the convergence issues are likely due to the same state/counties with small sample size and few outcome observations and lack of variability between exposure/outcome.

Going to fit what I have now, take a walk, and see what comes out. First thing to do is find cumulative effect, then lagged day effects.

```{r mixed mod results, warning=F, message=F, results="asis"}

# create a dataframe that contains cumulative estimates for each outcome
cumulative_df <- as_tibble(matrix(NA, nrow=0, ncol=4))
colnames(cumulative_df) <- c("outcome", "rel_risk", "lower_95", "upper_95")

# create a dataframe that contains the distributed lag estimates for each outcome
dist_lag_df <- as_tibble(matrix(NA,  nrow = 0, ncol = 5))
colnames(dist_lag_df) <- c("outcome", "lag", "rel_risk", 
                                  "lower_95", "upper_95")

# loop through best fit df
for(j in 1:length(lag_spline_best_fit$outcome)){
    
    # create empty matrix two empty matrices to fill
    cumul_val <- as_tibble(matrix(NA, nrow=1, ncol=4))
    colnames(cumul_val) <- c("outcome", "rel_risk", "lower_95", "upper_95")
    
    dl_val <- as_tibble(matrix(NA, nrow=8, ncol=5))
    colnames(dl_val) <- c("outcome", "lag", "rel_risk", 
                                  "lower_95", "upper_95")
    
    # easiest to set out the parameters from the start
    outcome <- lag_spline_best_fit$outcome[j]
    degree_freedom <- as.numeric(lag_spline_best_fit$df[j])

    # define basis b using natural spline function
    b <- ns(0:7, df = degree_freedom, intercept = T)
    
    # multiply lagged pm matrix by basis
    pm_b <- smk_matrix %*% b
    
    # fit mixed model
    mod <- glmer(as.formula(paste0(outcome, "~pm_b+wrf_temp_sd+
      (1|state_county)+offset(log(population))")), meta_ts2, 
      family="poisson", control = glmerControl(optimizer = "bobyqa"))

    mod
    # output distributed lag beta parameters; this will change based on DF
    dlparms <- mod@beta[2:(1+degree_freedom)]
    # estimate distributed lag values for each day
    dl_estimates <- data.frame(estimate = b %*% dlparms)
    
    # covariance matrix for knots (need to convert to matrix object)
    cov_mat <- as.matrix(vcov(mod)[2:(1+degree_freedom),2:(1+degree_freedom)]) 
    # estimate variance of splines
    dl_var <- b%*%cov_mat%*%t(b)
    # calculate standard error for each lag value
    dl_estimates$stderr <- sqrt(diag(dl_var))
    # calculate lower and upper bounds
    dl_estimates$lower_bound <- dl_estimates$estimate+dl_estimates$stderr*qt(1-0.975,
      df=df.residual(mod))  
    dl_estimates$upper_bound <- dl_estimates$estimate+dl_estimates$stderr*qt(0.975,
      df=df.residual(mod))  
    
    # estimate cumulative effect ----
    cumulative <- sum(dl_estimates$estimate)
    # estimate cumulative effect stnd error
    cumulative_se <- sqrt(sum(dl_var))
    # estimate cumulative CI
    cumulative_ci <- cumulative+cumulative_se*qt(c(1-0.975,0.975), 
                                                 df=df.residual(mod))
    # fill tibbles and bind in to larger dataframes ----
    # outcome
    cumul_val[1,1] <- outcome
    # cumulative rel risk
    cumul_val[1,2] <- exp(cumulative)
    # cumulative 95% CI
    cumul_val[1,3:4] <- exp(cumulative_ci)
    # bind to cumulative dataframe
    cumulative_df <- rbind(cumulative_df, cumul_val)

    # lag vals (this step is redundant)
    dl_val[,1] <- outcome
    dl_val[,2] <- 0:7
    dl_val[,3] <- exp(dl_estimates[,1])
    dl_val[,4] <- exp(dl_estimates[,3])
    dl_val[,5] <- exp(dl_estimates[,4])
    
    # bind in with overall df
    dist_lag_df <- rbind(dist_lag_df, dl_val)
} # end loop

# print table of cumulative effects
knitr::kable(cumulative_df, caption = paste0("Cumulative Effect for a 10 ug/m^3 increase in GWR smoke lagged days 0-7 mixed model approach"))

# plot distributed lag effects
ggplot(dist_lag_df, aes(x=lag, y=rel_risk)) +
  geom_line(colour = "blue") +
  geom_ribbon(aes(ymin = lower_95, ymax = upper_95), fill = "blue", alpha = 0.3) + 
  scale_y_continuous(limits = c(0.8, 1.2)) +
  scale_x_continuous(breaks = c(seq(0, 7,by=1))) +
  geom_hline(yintercept = 1, linetype = 2, colour = "red") +
  facet_wrap(~ outcome) +
  ggtitle(paste0("Distributed lag association between a 10 ug/m^3",
                 " increase in GWR smoke PM2.5 and cardiopulmonary outcomes")) +
  ylab("Relative Risk for a 10 ug/m^3 increase in GWR smoke (fixed)") +
  xlab("Lagged Days") +
  theme_bw()
```

Some results are similar to the two-stage approach. Ander suggests only interpreting lagged days if the cumulative effect is meaningful. Only asthma and all respiratory have significant cumulative effects. I think I might leave out broken arm for presentations to avoid confusion as there may be a protective effect 5 to 6 days after smoke exposure, which may make sense biologically if people do not go outside. There may be more meaningful associations the more states/years we include.